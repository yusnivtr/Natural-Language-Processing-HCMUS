{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusnivtr/Natural-Language-Processing-HCMUS/blob/main/skipgram_cbow_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbWNBFv5ineh"
      },
      "source": [
        "# Word embedding and one-hot encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfCcod1xoDoF"
      },
      "source": [
        "## One-hot encoding\n",
        "\n",
        "> One-hot encoding is the process of turning categorical factors into a numerical structure that machine learning algorithms can readily process. It functions by representing each category in a feature as a binary vector of 1s and 0s, with the vector's size equivalent to the number of potential categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tl5VgYs_ipju"
      },
      "outputs": [],
      "source": [
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVqliCz4njHW"
      },
      "source": [
        "### One-hot integer encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf3v1gLInkwK",
        "outputId": "eb8d5f59-0864-43fe-83c5-52983f6cc01b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[0 0 2 0 1 1 2 0 2 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(np.array(data))\n",
        "\n",
        "print(data)\n",
        "print(integer_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_eLAP-Mn9Dp"
      },
      "source": [
        "### One-hot binary encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8zKywxqn7-8",
        "outputId": "29aba087-401b-42c4-af70-6a16c21bdf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "  (np.int32(0), np.int32(0))\t1.0\n",
            "  (np.int32(1), np.int32(0))\t1.0\n",
            "  (np.int32(2), np.int32(2))\t1.0\n",
            "  (np.int32(3), np.int32(0))\t1.0\n",
            "  (np.int32(4), np.int32(1))\t1.0\n",
            "  (np.int32(5), np.int32(1))\t1.0\n",
            "  (np.int32(6), np.int32(2))\t1.0\n",
            "  (np.int32(7), np.int32(0))\t1.0\n",
            "  (np.int32(8), np.int32(2))\t1.0\n",
            "  (np.int32(9), np.int32(1))\t1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded_data = one_hot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "print(data)\n",
        "print(onehot_encoded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj-KxAzsyN67"
      },
      "source": [
        "## Problem 1\n",
        "What are the limitations of one-hot encoding?\n",
        "\n",
        "- Không thể nắm bắt mối quan hệ ngữ nghĩa giữa các từ. Trong biểu diễn one-hot, sự khác biệt giữa \"mèo\" và \"chó\" cũng giống như sự khác biệt giữa \"mèo\" và \"giường\".\n",
        "- Đây là một kiểu biểu diễn thưa thớt (sparse representation) và có số chiều lớn (high-dimensional), bằng với kích thước của từ vựng |V|. Điều này có thể dẫn đến vấn đề về hiệu suất tính toán.\n",
        "- Không linh hoạt trong việc xử lý các từ mới. Khi gặp một từ mới, cần phải gán cho nó một chỉ mục mới, điều này có thể làm thay đổi kích thước của biểu diễn và gây ra vấn đề cho các hệ thống NLP hiện có.\n",
        "- Hầu như không chứa bất kỳ thông tin ngữ nghĩa nào về các từ, mà chỉ đơn giản là phân biệt chúng với nhau.\n",
        "- Khi được dùng để biểu diễn câu hoặc tài liệu (dưới dạng tổng vector one-hot), phương pháp này bỏ qua hoàn toàn thứ tự của các từ. Do đó, các câu hoặc tài liệu khác nhau có thể có cùng một biểu diễn nếu chúng chứa cùng một tập hợp các từ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7OsPVqbrbqr"
      },
      "source": [
        "## Word embedding\n",
        "\n",
        "ELI5 for word embeddings\n",
        "> The word embeddings can be thought of as a child’s understanding of the words. Initially, the word embeddings are randomly initialized and they don’t make any sense, just like the baby has no understanding of different words. It’s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "df8-lpXbtHe2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "W2EvpsPVvgmW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfvwcF5Uz_-A"
      },
      "source": [
        "### Unigram transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vPNwu0lWymvJ"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "from typing import List\n",
        "\n",
        "def ngrams_transform(document: List[str],\n",
        "                     n_gram: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    N-grams transformations for a given text\n",
        "\n",
        "    Args:\n",
        "    document (List[str]) -- The document to-be-processed\n",
        "    n_gram   (int)       -- Number of grams\n",
        "\n",
        "    Returns:\n",
        "    A list of string after n-grams processed\n",
        "    \"\"\"\n",
        "\n",
        "    n_grams_list = []\n",
        "    for sentence in document:\n",
        "        tokens = sentence.split()\n",
        "        tokens = [token.lower() for token in tokens]\n",
        "        n_grams = ngrams(tokens, n_gram)\n",
        "        n_grams_list.extend([' '.join(gram) for gram in n_grams])\n",
        "    return n_grams_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPIUAu_-gX5",
        "outputId": "471541d3-16f4-47ee-ddcc-f471fdc57566"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'document.',\n",
              " 'this',\n",
              " 'document',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'document.',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'one.',\n",
              " 'is',\n",
              " 'this',\n",
              " 'the',\n",
              " 'first',\n",
              " 'document?']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_grams_list = ngrams_transform(corpus,\n",
        "                                n_gram=1)\n",
        "n_grams_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w99wG6cn0Dap",
        "outputId": "37ee7987-7eaf-4ba1-b65f-754e380e343f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example text tensor: tensor([10,  5,  8,  4,  2, 10,  1,  5,  8,  7,  2,  0, 10,  5,  8,  9,  6,  5,\n",
            "        10,  8,  4,  3])\n",
            "Shape of example text tensor: torch.Size([22])\n"
          ]
        }
      ],
      "source": [
        "# Integer label for the given corpus\n",
        "label_encoder = LabelEncoder()\n",
        "corpus_vector = label_encoder.fit_transform(np.array(n_grams_list))\n",
        "\n",
        "# Tensorize the input vector\n",
        "example_text_tensor = torch.Tensor(corpus_vector).to(dtype=torch.long)\n",
        "print(f\"Example text tensor: {example_text_tensor}\")\n",
        "print(f\"Shape of example text tensor: {example_text_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uVmzQtLvs66"
      },
      "source": [
        "### Create an example for embedding function to map from a word dimension to a lower dimensional space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "OJTdk0_drc0L"
      },
      "outputs": [],
      "source": [
        "num_vocab = 22 # number of vocabulary\n",
        "num_dimension = 50 # dimensional embeddings\n",
        "\n",
        "# Declare the mapping function\n",
        "example_embedding_function = nn.Embedding(num_vocab, num_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmTjA3-4uGKG",
        "outputId": "29df29ce-6427-4231-9eb7-b2132703adbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: torch.Size([22, 50])\n"
          ]
        }
      ],
      "source": [
        "example_output_tensor = example_embedding_function(example_text_tensor)\n",
        "print(f\"Embedding shape: {example_output_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oElUUcX2ZyDX"
      },
      "source": [
        "# Word2vec\n",
        "\n",
        "\n",
        "* Word2vec is a **class of models** that represents a word in a large text corpus as a vector in n-dimensional space(or n-dimensional feature space) bringing similar words closer to each other.\n",
        "\n",
        "\n",
        "\n",
        "* Word2vec is a simple yet popular model to construct representating embedding for words from a representation space to a much lower dimensional space (compared to the respective number of words in a dictionary).\n",
        "\n",
        "\n",
        "\n",
        "* Word2Vec has two neural network-based variants, which are:\n",
        "\n",
        "    * Continuous Bag of Words (CBOW)\n",
        "    * Skip-gram.\n",
        "![](https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-2048x1075.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwAVXC0V8vWA"
      },
      "source": [
        "## Continuous Bag of words (CBOW)\n",
        "\n",
        "* The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
        "\n",
        "* CBOW is modelled as follows:\n",
        "    * Given a target word $w_i$ and an $N$ context window on each side, $w_{i-1}, \\cdots, w_{i-N}$ and $w_{i+1},\\cdots, w_{i+N}$, referring to all context words collectively as $C$.\n",
        "\n",
        "    * CBOW tries to minimize the objective function:\n",
        "\n",
        "$$\n",
        "-\\log p(w_i|C) = -\\log\\text{Softmax}\\left(A\\left(\\sum_{w\\in C}q_w\\right)+b\\right)\n",
        "$$\n",
        "\n",
        "where $q_w$ is the embedding of word $w$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSvo8dncDUvv",
        "outputId": "b0d917d9-fe82-4a1d-dc5e-b6d1039bab4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# N = 2 according to the definition\n",
        "CONTEXT_SIZE = 2\n",
        "\n",
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "corpus = corpus.split()\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3MOE0WmFBd-"
      },
      "source": [
        "### Create an integer mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4eHuiF8yOj",
        "outputId": "4ffbd020-0616-4a35-d4d5-a71fcafe578f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'evolution': 0,\n",
              " 'rules': 1,\n",
              " 'computer': 2,\n",
              " 'spells.': 3,\n",
              " 'idea': 4,\n",
              " 'directed': 5,\n",
              " 'The': 6,\n",
              " 'As': 7,\n",
              " 'about': 8,\n",
              " 'direct': 9,\n",
              " 'create': 10,\n",
              " 'computers.': 11,\n",
              " 'processes.': 12,\n",
              " 'spirits': 13,\n",
              " 'Computational': 14,\n",
              " 'conjure': 15,\n",
              " 'computational': 16,\n",
              " 'we': 17,\n",
              " 'that': 18,\n",
              " 'programs': 19,\n",
              " 'the': 20,\n",
              " 'with': 21,\n",
              " 'are': 22,\n",
              " 'inhabit': 23,\n",
              " 'In': 24,\n",
              " 'to': 25,\n",
              " 'called': 26,\n",
              " 'process.': 27,\n",
              " 'data.': 28,\n",
              " 'pattern': 29,\n",
              " 'is': 30,\n",
              " 'a': 31,\n",
              " 'process': 32,\n",
              " 'We': 33,\n",
              " 'they': 34,\n",
              " 'beings': 35,\n",
              " 'program.': 36,\n",
              " 'things': 37,\n",
              " 'by': 38,\n",
              " 'People': 39,\n",
              " 'effect,': 40,\n",
              " 'our': 41,\n",
              " 'processes': 42,\n",
              " 'of': 43,\n",
              " 'abstract': 44,\n",
              " 'evolve,': 45,\n",
              " 'study': 46,\n",
              " 'other': 47,\n",
              " 'manipulate': 48}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Integer word mapping\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqrPWRsTE5zA"
      },
      "source": [
        "### Build context according to the given corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgBAHKy5CoJ7",
        "outputId": "7f8fa471-f3e3-40cf-bf52-fa47c2607486"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(['are', 'We', 'to', 'study'], 'about'),\n",
              " (['about', 'are', 'study', 'the'], 'to'),\n",
              " (['to', 'about', 'the', 'idea'], 'study'),\n",
              " (['study', 'to', 'idea', 'of'], 'the'),\n",
              " (['the', 'study', 'of', 'a'], 'idea'),\n",
              " (['idea', 'the', 'a', 'computational'], 'of'),\n",
              " (['of', 'idea', 'computational', 'process.'], 'a'),\n",
              " (['a', 'of', 'process.', 'Computational'], 'computational'),\n",
              " (['computational', 'a', 'Computational', 'processes'], 'process.'),\n",
              " (['process.', 'computational', 'processes', 'are'], 'Computational'),\n",
              " (['Computational', 'process.', 'are', 'abstract'], 'processes'),\n",
              " (['processes', 'Computational', 'abstract', 'beings'], 'are'),\n",
              " (['are', 'processes', 'beings', 'that'], 'abstract'),\n",
              " (['abstract', 'are', 'that', 'inhabit'], 'beings'),\n",
              " (['beings', 'abstract', 'inhabit', 'computers.'], 'that'),\n",
              " (['that', 'beings', 'computers.', 'As'], 'inhabit'),\n",
              " (['inhabit', 'that', 'As', 'they'], 'computers.'),\n",
              " (['computers.', 'inhabit', 'they', 'evolve,'], 'As'),\n",
              " (['As', 'computers.', 'evolve,', 'processes'], 'they'),\n",
              " (['they', 'As', 'processes', 'manipulate'], 'evolve,'),\n",
              " (['evolve,', 'they', 'manipulate', 'other'], 'processes'),\n",
              " (['processes', 'evolve,', 'other', 'abstract'], 'manipulate'),\n",
              " (['manipulate', 'processes', 'abstract', 'things'], 'other'),\n",
              " (['other', 'manipulate', 'things', 'called'], 'abstract'),\n",
              " (['abstract', 'other', 'called', 'data.'], 'things'),\n",
              " (['things', 'abstract', 'data.', 'The'], 'called'),\n",
              " (['called', 'things', 'The', 'evolution'], 'data.'),\n",
              " (['data.', 'called', 'evolution', 'of'], 'The'),\n",
              " (['The', 'data.', 'of', 'a'], 'evolution'),\n",
              " (['evolution', 'The', 'a', 'process'], 'of'),\n",
              " (['of', 'evolution', 'process', 'is'], 'a'),\n",
              " (['a', 'of', 'is', 'directed'], 'process'),\n",
              " (['process', 'a', 'directed', 'by'], 'is'),\n",
              " (['is', 'process', 'by', 'a'], 'directed'),\n",
              " (['directed', 'is', 'a', 'pattern'], 'by'),\n",
              " (['by', 'directed', 'pattern', 'of'], 'a'),\n",
              " (['a', 'by', 'of', 'rules'], 'pattern'),\n",
              " (['pattern', 'a', 'rules', 'called'], 'of'),\n",
              " (['of', 'pattern', 'called', 'a'], 'rules'),\n",
              " (['rules', 'of', 'a', 'program.'], 'called'),\n",
              " (['called', 'rules', 'program.', 'People'], 'a'),\n",
              " (['a', 'called', 'People', 'create'], 'program.'),\n",
              " (['program.', 'a', 'create', 'programs'], 'People'),\n",
              " (['People', 'program.', 'programs', 'to'], 'create'),\n",
              " (['create', 'People', 'to', 'direct'], 'programs'),\n",
              " (['programs', 'create', 'direct', 'processes.'], 'to'),\n",
              " (['to', 'programs', 'processes.', 'In'], 'direct'),\n",
              " (['direct', 'to', 'In', 'effect,'], 'processes.'),\n",
              " (['processes.', 'direct', 'effect,', 'we'], 'In'),\n",
              " (['In', 'processes.', 'we', 'conjure'], 'effect,'),\n",
              " (['effect,', 'In', 'conjure', 'the'], 'we'),\n",
              " (['we', 'effect,', 'the', 'spirits'], 'conjure'),\n",
              " (['conjure', 'we', 'spirits', 'of'], 'the'),\n",
              " (['the', 'conjure', 'of', 'the'], 'spirits'),\n",
              " (['spirits', 'the', 'the', 'computer'], 'of'),\n",
              " (['of', 'spirits', 'computer', 'with'], 'the'),\n",
              " (['the', 'of', 'with', 'our'], 'computer'),\n",
              " (['computer', 'the', 'our', 'spells.'], 'with')]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "\n",
        "for i in range(CONTEXT_SIZE, len(corpus) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [corpus[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [corpus[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = corpus[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbFTMNAGWygb"
      },
      "source": [
        "### Problem 2\n",
        "Name at least 2 limitations at this context construction step? Explain your answers.\n",
        "\n",
        "**1. Mất dữ liệu ở phần đầu và cuối corpus**\n",
        "\n",
        "Do vòng lặp chỉ chạy từ `CONTEXT_SIZE` đến `len(corpus) - CONTEXT_SIZE`, các phần tử ở đầu và cuối corpus (trong phạm vi `CONTEXT_SIZE`) sẽ không bao giờ được chọn làm target. Ví dụ, nếu `CONTEXT_SIZE = 2`, các từ ở vị trí 0, 1 và 2 phần tử cuối cùng sẽ bị loại bỏ. Điều này làm giảm lượng dữ liệu huấn luyện, đặc biệt nghiêm trọng nếu corpus ngắn hoặc `CONTEXT_SIZE` lớn.\n",
        "\n",
        "**2. Kết hợp cả ngữ cảnh trước và sau có thể gây nhiễu thông tin**\n",
        "\n",
        "Việc trộn cả từ trước và sau vị trí mục tiêu (target) vào context có thể không phù hợp với các mô hình yêu cầu thứ tự nghiêm ngặt (ví dụ: mô hình dự đoán từ tiếp theo chỉ dựa trên các từ trước). Hơn nữa, việc bao gồm cả từ phía sau có thể  tiết lộ thông tin về từ mục tiêu, dẫn đến overfitting (mô hình học vẹt thay vì hiểu bối cảnh thực sự)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYMlC5FSHSqP"
      },
      "source": [
        "### Vectorize context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9aRCDPmYEnMg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sample:  ['are', 'We', 'to', 'study']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([ 9,  5, 46, 41])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp = np.array([item for item in vocab])\n",
        "encoder = LabelEncoder()\n",
        "idx = encoder.fit_transform(tmp)\n",
        "word_to_idx = dict(zip(tmp, idx))\n",
        "\n",
        "def make_context_vector(context: List[str],\n",
        "                        word_to_idx: dict) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to map a word context vector into a torch tensor\n",
        "\n",
        "    Args:\n",
        "    context (List[str]) -- A context (including individual n-grams tokens)\n",
        "    word_to_idx (dict)  -- A functionto map a word into its respective integer\n",
        "\n",
        "    Returns:\n",
        "    A pytorch tensor including a list of mapped word\n",
        "\n",
        "    Example:\n",
        "    ['are', 'We', 'to', 'study'] --> tensor([40, 22, 27, 47])\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "    vector = [word_to_idx[word] for word in context]\n",
        "    vector = torch.tensor(vector, dtype=torch.long)\n",
        "    return vector\n",
        "\n",
        "\n",
        "    ### END YOUR CODE HERE ###\n",
        "    \n",
        "# Functional test\n",
        "print(\"Example sample: \", data[0][0])\n",
        "make_context_vector(data[0][0], word_to_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmf-xZkEFOfZ",
        "outputId": "96784ab6-8c41-4bdf-dacb-f23245314411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sample:  ['are', 'We', 'to', 'study']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([ 9,  5, 46, 41])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Functional test\n",
        "print(\"Example sample: \", data[0][0])\n",
        "make_context_vector(data[0][0], word_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h_4P68vHVOW"
      },
      "source": [
        "### CBOW model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ViLponY1HZoD"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Neural weight initialization\n",
        "        nn.init.xavier_normal_(self.embedding_layer.weight)\n",
        "        nn.init.xavier_normal_(self.linear_layer.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Function to conduct forward passing\n",
        "        \"\"\"\n",
        "        embedding = self.embedding_layer(inputs)\n",
        "        embedding = torch.sum(embedding, dim=1)\n",
        "        output = self.linear_layer(embedding)\n",
        "        output_softmax = F.log_softmax(output, dim=1)\n",
        "        return output_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_vTUt8aJiEn",
        "outputId": "d9600ad7-9c9e-48b1-8bb9-be49a4863c9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embedding_layer): Embedding(49, 10)\n",
              "  (linear_layer): Linear(in_features=10, out_features=49, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cbow_model = CBOW(vocab_size=vocab_size,\n",
        "                  embed_dim=10)\n",
        "\n",
        "# Enable gradient for model training\n",
        "cbow_model.train()\n",
        "cbow_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9aXrzRCKQel"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5tYJoeKMmuz"
      },
      "source": [
        "#### Hyperparameters and training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "UxbOAYCDLlrd"
      },
      "outputs": [],
      "source": [
        "num_epochs: int = 5\n",
        "learning_rate: float = 5e-2\n",
        "optimizer: torch.optim = torch.optim.Adam(cbow_model.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "loss_function = nn.NLLLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgn6aHDsWk2r"
      },
      "source": [
        "#### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bv0CVQZKS7W",
        "outputId": "9670abaf-b513-460f-c129-0764a38009a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Loss: 3.84928560256958\n",
            "Epoch 2/5\n",
            "Loss: 3.1683542728424072\n",
            "Epoch 3/5\n",
            "Loss: 2.5093705654144287\n",
            "Epoch 4/5\n",
            "Loss: 1.7346961498260498\n",
            "Epoch 5/5\n",
            "Loss: 0.8970910310745239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-59-97ca5e4a0b37>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
            "<ipython-input-59-97ca5e4a0b37>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Construct input and target tensor\n",
        "    input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
        "    input_vector = input_vector.unsqueeze(0)\n",
        "    target_vector = target_vector.unsqueeze(0)\n",
        "\n",
        "    # Join whole data into 1 tensor set\n",
        "    for idx in range(1, len(data)):\n",
        "        input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n",
        "        target_tensor = torch.tensor(word_to_idx[data[idx][1]]).unsqueeze(0)\n",
        "        torch.cat((input_vector, input_tensor), 0)\n",
        "        torch.cat((target_vector, target_tensor), 0)\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "    cbow_model.zero_grad()\n",
        "\n",
        "    # Forward passing\n",
        "    log_probabilities = cbow_model(input_vector)\n",
        "\n",
        "    # Evaluate loss\n",
        "    loss = loss_function(log_probabilities, target_vector)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "    print(\"Loss:\", epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DrF_htiSXmI"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3SBZo4LSDP5",
        "outputId": "de69eb87-1f11-42a8-a44f-5c2d0d3d12f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2, 35, 47, 16]])\n",
            "torch.Size([1, 4])\n",
            "Context: ['In', 'processes.', 'we', 'conjure']\n",
            "Prediction: As\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-107-a022f23a6ee1>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # No gradient update in inference\n",
        "    context = ['In', 'processes.', 'we', 'conjure']\n",
        "\n",
        "    # Vectorize input from text to numeric type\n",
        "    input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n",
        "    # Model makes prediction\n",
        "    output_tensor = cbow_model(input_tensor)\n",
        "    # Get the item id with the highest probability\n",
        "    prediction = torch.argmax(output_tensor).detach().tolist()\n",
        "    # Query the respective word from the given item id\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = key_list[prediction]\n",
        "\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-118-f872ab30713d>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 4])"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
        "# input_vector.unsqueeze(-1)\n",
        "# target_vector = target_vector.unsqueeze(0)\n",
        "input_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9FwHWmGaM3S"
      },
      "source": [
        "## Skip-gram\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningcoban.com/tabml_book/_images/word2vec2.png\">\n",
        "</center>\n",
        "\n",
        "- Skip gram is based on the distributional hypothesis where words with similar distribution is considered to have similar meanings. Researchers of skip gram suggested a model with less parameters along with the novel methods to make optimization step more efficient.\n",
        "\n",
        "- Vanilla SkipGram model:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/a1d083c872e848787cb572a73d97f2c24947a374/5-Figure1-1.png\" scale=70%>\n",
        "</center>\n",
        "\n",
        "- Main idea is to optimize model so that if it is queried with a word, it should correctly guess all the context (context = 2 in the figure) words. That is,\n",
        "$$\n",
        "y=\\sigma(Ux)\n",
        "$$\n",
        "    - where $x$, $y$ are one-hot encoded word vector, $U$ is the embedding matrix, and $\\sigma(\\cdot)$ is the softmax function.\n",
        "\n",
        "With the same dataset, training set for skip gram can be much larger than that of NPLM since it can have $2c$ samples $\\left(w_t:w_{t-c}, ...,w_t:w_{t-1},w_t:w_{t+1},...,w_{t+c}\\right)$ while other n-gram based models have one $\\left((w_{t-c},...w_{t-1},w_{t+1},...,w_{t+c}):w_t\\right)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "b9XRCWEvEOWC"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'As',\n",
              " 'Computational',\n",
              " 'In',\n",
              " 'People',\n",
              " 'The',\n",
              " 'We',\n",
              " 'a',\n",
              " 'about',\n",
              " 'abstract',\n",
              " 'are',\n",
              " 'beings',\n",
              " 'by',\n",
              " 'called',\n",
              " 'computational',\n",
              " 'computer',\n",
              " 'computers.',\n",
              " 'conjure',\n",
              " 'create',\n",
              " 'data.',\n",
              " 'direct',\n",
              " 'directed',\n",
              " 'effect,',\n",
              " 'evolution',\n",
              " 'evolve,',\n",
              " 'idea',\n",
              " 'inhabit',\n",
              " 'is',\n",
              " 'manipulate',\n",
              " 'of',\n",
              " 'other',\n",
              " 'our',\n",
              " 'pattern',\n",
              " 'process',\n",
              " 'process.',\n",
              " 'processes',\n",
              " 'processes.',\n",
              " 'program.',\n",
              " 'programs',\n",
              " 'rules',\n",
              " 'spells.',\n",
              " 'spirits',\n",
              " 'study',\n",
              " 'that',\n",
              " 'the',\n",
              " 'they',\n",
              " 'things',\n",
              " 'to',\n",
              " 'we',\n",
              " 'with'}"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_vocab(corpus):\n",
        "    words = corpus.split()\n",
        "    return set(words)\n",
        "\n",
        "vocab = create_vocab(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1dwgXNxZ_Lv"
      },
      "outputs": [],
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model construction\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        ### START YOUR CODE HERE ###\n",
        "        # Declare embedding function u and v\n",
        "        # with given vocab size and embed dim using nn.Embedding\n",
        "        self.v_embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
        "        self.u_embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "        # Network weight initialization with Xavier initialization\n",
        "        nn.init.xavier_normal_(self.v_embedding_layer.weight)\n",
        "        nn.init.xavier_normal_(self.u_embedding_layer.weight)\n",
        "\n",
        "        ### END YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, center_words, context):\n",
        "        \"\"\"\n",
        "        Function to perform forward passing\n",
        "        \"\"\"\n",
        "        v_embedding = self.v_embedding_layer(center_words)\n",
        "        u_embedding = self.u_embedding_layer(context)\n",
        "\n",
        "        score = torch.mul(v_embedding, u_embedding)\n",
        "        score = torch.sum(score, dim=-1)\n",
        "        log_score = F.logsigmoid(score)\n",
        "        return log_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ls_wI-7nS1",
        "outputId": "f69f5496-261f-48c2-8ead-f4b20dca100f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SkipGramModel(\n",
              "  (v_embedding_layer): Embedding(49, 128)\n",
              "  (u_embedding_layer): Embedding(49, 128)\n",
              ")"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "skipgram_model = SkipGramModel(vocab_size=vocab_size,\n",
        "                               embed_dim=128)\n",
        "\n",
        "skipgram_model.train()\n",
        "skipgram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiBCaJJJCMr_"
      },
      "source": [
        "### Prepare training data to match the format of SkipGram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5YlRPTuaCQBS"
      },
      "outputs": [],
      "source": [
        "def gather_training_data(corpus,\n",
        "                         word_to_idx: dict,\n",
        "                         context_size: int):\n",
        "    \"\"\"\n",
        "    This function is to transform the given corpus\n",
        "    into the correct format for SkipGram to serve as its input\n",
        "    \"\"\"\n",
        "\n",
        "    training_data = []\n",
        "    all_vocab_indices = list(range(len(word_to_idx)))\n",
        "\n",
        "    split_text = corpus.split('\\n')\n",
        "\n",
        "    # For each sentence\n",
        "    for sentence in split_text:\n",
        "        indices = []\n",
        "        indices = [word_to_idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "        # For each word treated as center word\n",
        "        for center_word_pos in range(len(indices)):\n",
        "\n",
        "            # For each window  position\n",
        "            for w in range(-context_size, context_size+1):\n",
        "                context_word_pos = center_word_pos + w\n",
        "\n",
        "                # Make sure we dont jump out of the sentence\n",
        "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                    continue\n",
        "\n",
        "                context_word_idx = indices[context_word_pos]\n",
        "                center_word_idx  = indices[center_word_pos]\n",
        "\n",
        "                # Same words might be present in the close vicinity of each other. we want to avoid such cases\n",
        "                if center_word_idx == context_word_idx:\n",
        "                    continue\n",
        "\n",
        "                training_data.append([center_word_idx, context_word_idx])\n",
        "\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVjk1oJkDPXq",
        "outputId": "5c6aa12d-ea15-4ced-8254-c0d2d0687714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5,  9],\n",
              "        [ 5,  7],\n",
              "        [ 9,  5],\n",
              "        [ 9,  7],\n",
              "        [ 9, 46],\n",
              "        [ 7,  5],\n",
              "        [ 7,  9],\n",
              "        [ 7, 46],\n",
              "        [ 7, 41],\n",
              "        [46,  9],\n",
              "        [46,  7],\n",
              "        [46, 41],\n",
              "        [46, 43],\n",
              "        [41,  7],\n",
              "        [41, 46],\n",
              "        [41, 43],\n",
              "        [41, 24],\n",
              "        [43, 46],\n",
              "        [43, 41],\n",
              "        [43, 24],\n",
              "        [43, 28],\n",
              "        [24, 41],\n",
              "        [24, 43],\n",
              "        [24, 28],\n",
              "        [24,  6],\n",
              "        [28, 43],\n",
              "        [28, 24],\n",
              "        [28,  6],\n",
              "        [28, 13],\n",
              "        [ 6, 24],\n",
              "        [ 6, 28],\n",
              "        [ 6, 13],\n",
              "        [ 6, 33],\n",
              "        [13, 28],\n",
              "        [13,  6],\n",
              "        [13, 33],\n",
              "        [33,  6],\n",
              "        [33, 13],\n",
              "        [ 1, 34],\n",
              "        [ 1,  9],\n",
              "        [34,  1],\n",
              "        [34,  9],\n",
              "        [34,  8],\n",
              "        [ 9,  1],\n",
              "        [ 9, 34],\n",
              "        [ 9,  8],\n",
              "        [ 9, 10],\n",
              "        [ 8, 34],\n",
              "        [ 8,  9],\n",
              "        [ 8, 10],\n",
              "        [ 8, 42],\n",
              "        [10,  9],\n",
              "        [10,  8],\n",
              "        [10, 42],\n",
              "        [10, 25],\n",
              "        [42,  8],\n",
              "        [42, 10],\n",
              "        [42, 25],\n",
              "        [42, 15],\n",
              "        [25, 10],\n",
              "        [25, 42],\n",
              "        [25, 15],\n",
              "        [15, 42],\n",
              "        [15, 25],\n",
              "        [ 0, 44],\n",
              "        [ 0, 23],\n",
              "        [44,  0],\n",
              "        [44, 23],\n",
              "        [44, 34],\n",
              "        [23,  0],\n",
              "        [23, 44],\n",
              "        [23, 34],\n",
              "        [23, 27],\n",
              "        [34, 44],\n",
              "        [34, 23],\n",
              "        [34, 27],\n",
              "        [34, 29],\n",
              "        [27, 23],\n",
              "        [27, 34],\n",
              "        [27, 29],\n",
              "        [27,  8],\n",
              "        [29, 34],\n",
              "        [29, 27],\n",
              "        [29,  8],\n",
              "        [29, 45],\n",
              "        [ 8, 27],\n",
              "        [ 8, 29],\n",
              "        [ 8, 45],\n",
              "        [ 8, 12],\n",
              "        [45, 29],\n",
              "        [45,  8],\n",
              "        [45, 12],\n",
              "        [45, 18],\n",
              "        [12,  8],\n",
              "        [12, 45],\n",
              "        [12, 18],\n",
              "        [18, 45],\n",
              "        [18, 12],\n",
              "        [ 4, 22],\n",
              "        [ 4, 28],\n",
              "        [22,  4],\n",
              "        [22, 28],\n",
              "        [22,  6],\n",
              "        [28,  4],\n",
              "        [28, 22],\n",
              "        [28,  6],\n",
              "        [28, 32],\n",
              "        [ 6, 22],\n",
              "        [ 6, 28],\n",
              "        [ 6, 32],\n",
              "        [ 6, 26],\n",
              "        [32, 28],\n",
              "        [32,  6],\n",
              "        [32, 26],\n",
              "        [32, 20],\n",
              "        [26,  6],\n",
              "        [26, 32],\n",
              "        [26, 20],\n",
              "        [26, 11],\n",
              "        [20, 32],\n",
              "        [20, 26],\n",
              "        [20, 11],\n",
              "        [20,  6],\n",
              "        [11, 26],\n",
              "        [11, 20],\n",
              "        [11,  6],\n",
              "        [11, 31],\n",
              "        [ 6, 20],\n",
              "        [ 6, 11],\n",
              "        [ 6, 31],\n",
              "        [ 6, 28],\n",
              "        [31, 11],\n",
              "        [31,  6],\n",
              "        [31, 28],\n",
              "        [31, 38],\n",
              "        [28,  6],\n",
              "        [28, 31],\n",
              "        [28, 38],\n",
              "        [38, 31],\n",
              "        [38, 28],\n",
              "        [12,  6],\n",
              "        [12, 36],\n",
              "        [ 6, 12],\n",
              "        [ 6, 36],\n",
              "        [ 6,  3],\n",
              "        [36, 12],\n",
              "        [36,  6],\n",
              "        [36,  3],\n",
              "        [36, 17],\n",
              "        [ 3,  6],\n",
              "        [ 3, 36],\n",
              "        [ 3, 17],\n",
              "        [ 3, 37],\n",
              "        [17, 36],\n",
              "        [17,  3],\n",
              "        [17, 37],\n",
              "        [17, 46],\n",
              "        [37,  3],\n",
              "        [37, 17],\n",
              "        [37, 46],\n",
              "        [37, 19],\n",
              "        [46, 17],\n",
              "        [46, 37],\n",
              "        [46, 19],\n",
              "        [46, 35],\n",
              "        [19, 37],\n",
              "        [19, 46],\n",
              "        [19, 35],\n",
              "        [19,  2],\n",
              "        [35, 46],\n",
              "        [35, 19],\n",
              "        [35,  2],\n",
              "        [35, 21],\n",
              "        [ 2, 19],\n",
              "        [ 2, 35],\n",
              "        [ 2, 21],\n",
              "        [21, 35],\n",
              "        [21,  2],\n",
              "        [47, 16],\n",
              "        [47, 43],\n",
              "        [16, 47],\n",
              "        [16, 43],\n",
              "        [16, 40],\n",
              "        [43, 47],\n",
              "        [43, 16],\n",
              "        [43, 40],\n",
              "        [43, 28],\n",
              "        [40, 16],\n",
              "        [40, 43],\n",
              "        [40, 28],\n",
              "        [40, 43],\n",
              "        [28, 43],\n",
              "        [28, 40],\n",
              "        [28, 43],\n",
              "        [28, 14],\n",
              "        [43, 40],\n",
              "        [43, 28],\n",
              "        [43, 14],\n",
              "        [43, 48],\n",
              "        [14, 28],\n",
              "        [14, 43],\n",
              "        [14, 48],\n",
              "        [14, 30],\n",
              "        [48, 43],\n",
              "        [48, 14],\n",
              "        [48, 30],\n",
              "        [48, 39],\n",
              "        [30, 14],\n",
              "        [30, 48],\n",
              "        [30, 39],\n",
              "        [39, 48],\n",
              "        [39, 30]])"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data = gather_training_data(corpus,\n",
        "                                     word_to_idx,\n",
        "                                     context_size=2)\n",
        "training_data = torch.tensor(training_data).to(dtype=torch.long)\n",
        "training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWEIXRlg8zmg"
      },
      "source": [
        "### Hyperparamters and training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "wIgEYDKz8Sbu"
      },
      "outputs": [],
      "source": [
        "num_epochs: int = 200\n",
        "learning_rate: float = 5e-1\n",
        "optimizer: torch.optim = torch.optim.SGD(skipgram_model.parameters(),\n",
        "                                          lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmG0m3jK84EF"
      },
      "source": [
        "### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhGy9vBq85PX",
        "outputId": "fe2241aa-0ede-41d4-f0b0-91fcec8aa755"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ba32581e8f34bac876f539b84b5264a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/201 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/200\n",
            "#Epoch 0/200\n",
            "Loss: 0.7464172840118408\n",
            "Epoch 1/200\n",
            "Epoch 2/200\n",
            "Epoch 3/200\n",
            "Epoch 4/200\n",
            "Epoch 5/200\n",
            "Epoch 6/200\n",
            "Epoch 7/200\n",
            "Epoch 8/200\n",
            "Epoch 9/200\n",
            "Epoch 10/200\n",
            "Epoch 11/200\n",
            "Epoch 12/200\n",
            "Epoch 13/200\n",
            "Epoch 14/200\n",
            "Epoch 15/200\n",
            "Epoch 16/200\n",
            "Epoch 17/200\n",
            "Epoch 18/200\n",
            "Epoch 19/200\n",
            "Epoch 20/200\n",
            "Epoch 21/200\n",
            "Epoch 22/200\n",
            "Epoch 23/200\n",
            "Epoch 24/200\n",
            "Epoch 25/200\n",
            "Epoch 26/200\n",
            "Epoch 27/200\n",
            "Epoch 28/200\n",
            "Epoch 29/200\n",
            "Epoch 30/200\n",
            "Epoch 31/200\n",
            "Epoch 32/200\n",
            "Epoch 33/200\n",
            "Epoch 34/200\n",
            "Epoch 35/200\n",
            "Epoch 36/200\n",
            "Epoch 37/200\n",
            "Epoch 38/200\n",
            "Epoch 39/200\n",
            "Epoch 40/200\n",
            "Epoch 41/200\n",
            "Epoch 42/200\n",
            "Epoch 43/200\n",
            "Epoch 44/200\n",
            "Epoch 45/200\n",
            "Epoch 46/200\n",
            "Epoch 47/200\n",
            "Epoch 48/200\n",
            "Epoch 49/200\n",
            "Epoch 50/200\n",
            "#Epoch 50/200\n",
            "Loss: 0.004693763330578804\n",
            "Epoch 51/200\n",
            "Epoch 52/200\n",
            "Epoch 53/200\n",
            "Epoch 54/200\n",
            "Epoch 55/200\n",
            "Epoch 56/200\n",
            "Epoch 57/200\n",
            "Epoch 58/200\n",
            "Epoch 59/200\n",
            "Epoch 60/200\n",
            "Epoch 61/200\n",
            "Epoch 62/200\n",
            "Epoch 63/200\n",
            "Epoch 64/200\n",
            "Epoch 65/200\n",
            "Epoch 66/200\n",
            "Epoch 67/200\n",
            "Epoch 68/200\n",
            "Epoch 69/200\n",
            "Epoch 70/200\n",
            "Epoch 71/200\n",
            "Epoch 72/200\n",
            "Epoch 73/200\n",
            "Epoch 74/200\n",
            "Epoch 75/200\n",
            "Epoch 76/200\n",
            "Epoch 77/200\n",
            "Epoch 78/200\n",
            "Epoch 79/200\n",
            "Epoch 80/200\n",
            "Epoch 81/200\n",
            "Epoch 82/200\n",
            "Epoch 83/200\n",
            "Epoch 84/200\n",
            "Epoch 85/200\n",
            "Epoch 86/200\n",
            "Epoch 87/200\n",
            "Epoch 88/200\n",
            "Epoch 89/200\n",
            "Epoch 90/200\n",
            "Epoch 91/200\n",
            "Epoch 92/200\n",
            "Epoch 93/200\n",
            "Epoch 94/200\n",
            "Epoch 95/200\n",
            "Epoch 96/200\n",
            "Epoch 97/200\n",
            "Epoch 98/200\n",
            "Epoch 99/200\n",
            "Epoch 100/200\n",
            "#Epoch 100/200\n",
            "Loss: 0.0019527068361639977\n",
            "Epoch 101/200\n",
            "Epoch 102/200\n",
            "Epoch 103/200\n",
            "Epoch 104/200\n",
            "Epoch 105/200\n",
            "Epoch 106/200\n",
            "Epoch 107/200\n",
            "Epoch 108/200\n",
            "Epoch 109/200\n",
            "Epoch 110/200\n",
            "Epoch 111/200\n",
            "Epoch 112/200\n",
            "Epoch 113/200\n",
            "Epoch 114/200\n",
            "Epoch 115/200\n",
            "Epoch 116/200\n",
            "Epoch 117/200\n",
            "Epoch 118/200\n",
            "Epoch 119/200\n",
            "Epoch 120/200\n",
            "Epoch 121/200\n",
            "Epoch 122/200\n",
            "Epoch 123/200\n",
            "Epoch 124/200\n",
            "Epoch 125/200\n",
            "Epoch 126/200\n",
            "Epoch 127/200\n",
            "Epoch 128/200\n",
            "Epoch 129/200\n",
            "Epoch 130/200\n",
            "Epoch 131/200\n",
            "Epoch 132/200\n",
            "Epoch 133/200\n",
            "Epoch 134/200\n",
            "Epoch 135/200\n",
            "Epoch 136/200\n",
            "Epoch 137/200\n",
            "Epoch 138/200\n",
            "Epoch 139/200\n",
            "Epoch 140/200\n",
            "Epoch 141/200\n",
            "Epoch 142/200\n",
            "Epoch 143/200\n",
            "Epoch 144/200\n",
            "Epoch 145/200\n",
            "Epoch 146/200\n",
            "Epoch 147/200\n",
            "Epoch 148/200\n",
            "Epoch 149/200\n",
            "Epoch 150/200\n",
            "#Epoch 150/200\n",
            "Loss: 0.0011864814441651106\n",
            "Epoch 151/200\n",
            "Epoch 152/200\n",
            "Epoch 153/200\n",
            "Epoch 154/200\n",
            "Epoch 155/200\n",
            "Epoch 156/200\n",
            "Epoch 157/200\n",
            "Epoch 158/200\n",
            "Epoch 159/200\n",
            "Epoch 160/200\n",
            "Epoch 161/200\n",
            "Epoch 162/200\n",
            "Epoch 163/200\n",
            "Epoch 164/200\n",
            "Epoch 165/200\n",
            "Epoch 166/200\n",
            "Epoch 167/200\n",
            "Epoch 168/200\n",
            "Epoch 169/200\n",
            "Epoch 170/200\n",
            "Epoch 171/200\n",
            "Epoch 172/200\n",
            "Epoch 173/200\n",
            "Epoch 174/200\n",
            "Epoch 175/200\n",
            "Epoch 176/200\n",
            "Epoch 177/200\n",
            "Epoch 178/200\n",
            "Epoch 179/200\n",
            "Epoch 180/200\n",
            "Epoch 181/200\n",
            "Epoch 182/200\n",
            "Epoch 183/200\n",
            "Epoch 184/200\n",
            "Epoch 185/200\n",
            "Epoch 186/200\n",
            "Epoch 187/200\n",
            "Epoch 188/200\n",
            "Epoch 189/200\n",
            "Epoch 190/200\n",
            "Epoch 191/200\n",
            "Epoch 192/200\n",
            "Epoch 193/200\n",
            "Epoch 194/200\n",
            "Epoch 195/200\n",
            "Epoch 196/200\n",
            "Epoch 197/200\n",
            "Epoch 198/200\n",
            "Epoch 199/200\n",
            "Epoch 200/200\n",
            "#Epoch 200/200\n",
            "Loss: 0.0008376812329515815\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "for epoch in tqdm(range(num_epochs + 1)):\n",
        "    \"\"\"\n",
        "    Adapt the given CBOW training code for SkipGram\n",
        "    Following by the instruction comments, or you could do it on your own ;)\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ###\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    # Construct input and target tensor\n",
        "    inputs, targets = training_data[0][0], training_data[0][1]\n",
        "    inputs = inputs.unsqueeze(0)\n",
        "    targets = targets.unsqueeze(0)\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward passing\n",
        "    logsoftmax_prediction = skipgram_model(inputs, targets)\n",
        "\n",
        "    # Evaluate loss (Negative log likelihood)\n",
        "    loss = torch.mean(-1 * logsoftmax_prediction)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "\n",
        "    # Log result\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "        print(\"Loss:\", epoch_loss)\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8DX0BfFDsK"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "U2EcHAqVFEzb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: ['we']\n",
            "Prediction: directed\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    context = ['we']\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Based on the given inference code in the previous section, training code and the context\n",
        "    # Implement the inference flow from the given context to an output word\n",
        "    input = word_to_idx[context[0]]\n",
        "    input_tensor = torch.tensor(input).unsqueeze(0)\n",
        "    \n",
        "    u_embedding_context = skipgram_model.u_embedding_layer(input_tensor)\n",
        "    \n",
        "    all_v_embeddings = skipgram_model.v_embedding_layer.weight\n",
        "    \n",
        "    \n",
        "    similarity_scores = torch.matmul(all_v_embeddings,u_embedding_context.T)\n",
        "    \n",
        "    best_center_word_idx = torch.argmax(similarity_scores)\n",
        "\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = key_list[best_center_word_idx]\n",
        "    ### END YOUR CODE HERE ###\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sg4EaBpw-De"
      },
      "source": [
        "## Problem 3\n",
        "What are the differences between CBOW and Skip-gram?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Mục tiêu Dự đoán (Prediction Goal):**\n",
        "    *   **CBOW:** Dự đoán từ *mục tiêu* (target word) dựa trên các từ *ngữ cảnh* (context words) xung quanh nó. \n",
        "    *   **Skip-gram:** Dự đoán các từ *ngữ cảnh* xung quanh dựa trên một từ *mục tiêu* cho trước.\n",
        "\n",
        "2.  **Đầu vào và Đầu ra (Input & Output):**\n",
        "    *   **CBOW:**\n",
        "        *   *Đầu vào:* Vectors của các từ ngữ cảnh (thường được kết hợp lại, ví dụ: lấy trung bình).\n",
        "        *   *Đầu ra:* Phân phối xác suất trên toàn bộ từ vựng cho từ mục tiêu.\n",
        "    *   **Skip-gram:**\n",
        "        *   *Đầu vào:* Vector của từ mục tiêu.\n",
        "        *   *Đầu ra:* *Nhiều* phân phối xác suất trên toàn bộ từ vựng, mỗi cái cho một vị trí từ ngữ cảnh.\n",
        "\n",
        "3.  **Tốc độ Huấn luyện (Training Speed):**\n",
        "    *   **CBOW:** Thường huấn luyện *nhanh hơn* Skip-gram. Lý do là CBOW cập nhật trọng số dựa trên việc dự đoán một từ duy nhất từ nhiều từ ngữ cảnh (ít tác vụ dự đoán hơn cho mỗi mẫu huấn luyện).\n",
        "    *   **Skip-gram:** Thường huấn luyện *chậm hơn* CBOW. Do một phần là nó phải thực hiện nhiều dự đoán (cho mỗi từ ngữ cảnh) từ một từ mục tiêu duy nhất (nhiều tác vụ dự đoán hơn cho mỗi mẫu huấn luyện).\n",
        "\n",
        "4.  **Hiệu quả với Dữ liệu và Từ hiếm (Performance with Data and Rare Words):**\n",
        "    *   **CBOW:** Hoạt động tốt với các tập dữ liệu lớn. Việc lấy trung bình ngữ cảnh làm cho nó hiệu quả hơn với các từ xuất hiện thường xuyên (frequent words).\n",
        "    *   **Skip-gram:** Hoạt động tốt ngay cả với các tập dữ liệu nhỏ hơn. Quan trọng hơn, nó có xu hướng học được các biểu diễn tốt hơn cho các từ xuất hiện *ít thường xuyên* (rare words) vì nó xem xét từng cặp (từ mục tiêu, từ ngữ cảnh) một cách riêng lẻ, không bị \"pha loãng\" bởi việc lấy trung bình.\n",
        "\n",
        "5.  **Chất lượng Embedding (Embedding Quality):**\n",
        "    *   **Skip-gram:** Thường được cho là tạo ra các word embeddings có chất lượng cao hơn một chút, đặc biệt là trong việc nắm bắt các mối quan hệ ngữ nghĩa và cú pháp tinh tế, nhất là đối với từ hiếm.\n",
        "    *   **CBOW:** Vẫn tạo ra embeddings chất lượng tốt, đặc biệt hiệu quả về mặt tốc độ và bộ nhớ.\n",
        "\n",
        "**Tóm laij:**\n",
        "\n",
        "| Đặc điểm             | CBOW (Continuous Bag-of-Words)              | Skip-gram                                     |\n",
        "| :------------------- | :------------------------------------------ | :-------------------------------------------- |\n",
        "| **Mục tiêu**         | Dự đoán từ mục tiêu từ ngữ cảnh             | Dự đoán ngữ cảnh từ từ mục tiêu                |\n",
        "| **Cách tiếp cận**    | Ngữ cảnh -> Từ mục tiêu                     | Từ mục tiêu -> Ngữ cảnh                        |\n",
        "| **Tốc độ huấn luyện** | Nhanh hơn                                    | Chậm hơn                                     |\n",
        "| **Từ hiếm**          | Kém hiệu quả hơn                            | Hiệu quả hơn                                 |\n",
        "| **Tập dữ liệu lớn**  | Hiệu quả tốt                                | Vẫn tốt, nhưng chậm hơn                      |\n",
        "| **Chất lượng**       | Tốt, nhanh hơn                              | Thường tốt hơn một chút, đặc biệt từ hiếm     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
