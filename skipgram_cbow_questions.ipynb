{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusnivtr/Natural-Language-Processing-HCMUS/blob/main/skipgram_cbow_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embedding and one-hot encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sbWNBFv5ineh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding\n",
        "\n",
        "> One-hot encoding is the process of turning categorical factors into a numerical structure that machine learning algorithms can readily process. It functions by representing each category in a feature as a binary vector of 1s and 0s, with the vector's size equivalent to the number of potential categories."
      ],
      "metadata": {
        "id": "PfCcod1xoDoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']"
      ],
      "metadata": {
        "id": "tl5VgYs_ipju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot integer encoding"
      ],
      "metadata": {
        "id": "ZVqliCz4njHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(np.array(data))\n",
        "\n",
        "print(data)\n",
        "print(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf3v1gLInkwK",
        "outputId": "eb8d5f59-0864-43fe-83c5-52983f6cc01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[0 0 2 0 1 1 2 0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot binary encoding"
      ],
      "metadata": {
        "id": "C_eLAP-Mn9Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded_data = one_hot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "print(data)\n",
        "print(onehot_encoded_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8zKywxqn7-8",
        "outputId": "29aba087-401b-42c4-af70-6a16c21bdf21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "What are the limitations of one-hot encoding?"
      ],
      "metadata": {
        "id": "gj-KxAzsyN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embedding\n",
        "\n",
        "ELI5 for word embeddings\n",
        "> The word embeddings can be thought of as a child’s understanding of the words. Initially, the word embeddings are randomly initialized and they don’t make any sense, just like the baby has no understanding of different words. It’s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words.\""
      ],
      "metadata": {
        "id": "o7OsPVqbrbqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "df8-lpXbtHe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]"
      ],
      "metadata": {
        "id": "W2EvpsPVvgmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram transformation"
      ],
      "metadata": {
        "id": "UfvwcF5Uz_-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from typing import List\n",
        "\n",
        "def ngrams_transform(document: List[str],\n",
        "                     n_gram: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    N-grams transformations for a given text\n",
        "\n",
        "    Args:\n",
        "    document (List[str]) -- The document to-be-processed\n",
        "    n_gram   (int)       -- Number of grams\n",
        "\n",
        "    Returns:\n",
        "    A list of string after n-grams processed\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "vPNwu0lWymvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_grams_list = ngrams_transform(corpus,\n",
        "                                n_gram=1)\n",
        "n_grams_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPIUAu_-gX5",
        "outputId": "471541d3-16f4-47ee-ddcc-f471fdc57566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'document.',\n",
              " 'this',\n",
              " 'document',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'document.',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'one.',\n",
              " 'is',\n",
              " 'this',\n",
              " 'the',\n",
              " 'first',\n",
              " 'document?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Integer label for the given corpus\n",
        "label_encoder = LabelEncoder()\n",
        "corpus_vector = label_encoder.fit_transform(np.array(n_grams_list))\n",
        "\n",
        "# Tensorize the input vector\n",
        "example_text_tensor = torch.Tensor(corpus_vector).to(dtype=torch.long)\n",
        "print(f\"Example text tensor: {example_text_tensor}\")\n",
        "print(f\"Shape of example text tensor: {example_text_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w99wG6cn0Dap",
        "outputId": "37ee7987-7eaf-4ba1-b65f-754e380e343f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example text tensor: tensor([10,  5,  8,  4,  2, 10,  1,  5,  8,  7,  2,  0, 10,  5,  8,  9,  6,  5,\n",
            "        10,  8,  4,  3])\n",
            "Shape of example text tensor: torch.Size([22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an example for embedding function to map from a word dimension to a lower dimensional space"
      ],
      "metadata": {
        "id": "0uVmzQtLvs66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vocab = 22 # number of vocabulary\n",
        "num_dimension = 50 # dimensional embeddings\n",
        "\n",
        "# Declare the mapping function\n",
        "example_embedding_function = nn.Embedding(num_vocab, num_dimension)"
      ],
      "metadata": {
        "id": "OJTdk0_drc0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_output_tensor = example_embedding_function(example_text_tensor)\n",
        "print(f\"Embedding shape: {example_output_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmTjA3-4uGKG",
        "outputId": "29df29ce-6427-4231-9eb7-b2132703adbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([22, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec\n",
        "\n",
        "\n",
        "* Word2vec is a **class of models** that represents a word in a large text corpus as a vector in n-dimensional space(or n-dimensional feature space) bringing similar words closer to each other.\n",
        "\n",
        "\n",
        "\n",
        "* Word2vec is a simple yet popular model to construct representating embedding for words from a representation space to a much lower dimensional space (compared to the respective number of words in a dictionary).\n",
        "\n",
        "\n",
        "\n",
        "* Word2Vec has two neural network-based variants, which are:\n",
        "\n",
        "    * Continuous Bag of Words (CBOW)\n",
        "    * Skip-gram.\n",
        "![](https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-2048x1075.png)\n"
      ],
      "metadata": {
        "id": "oElUUcX2ZyDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag of words (CBOW)\n",
        "\n",
        "* The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
        "\n",
        "* CBOW is modelled as follows:\n",
        "    * Given a target word $w_i$ and an $N$ context window on each side, $w_{i-1}, \\cdots, w_{i-N}$ and $w_{i+1},\\cdots, w_{i+N}$, referring to all context words collectively as $C$.\n",
        "\n",
        "    * CBOW tries to minimize the objective function:\n",
        "\n",
        "$$\n",
        "-\\log p(w_i|C) = -\\log\\text{Softmax}\\left(A\\left(\\sum_{w\\in C}q_w\\right)+b\\right)\n",
        "$$\n",
        "\n",
        "where $q_w$ is the embedding of word $w$."
      ],
      "metadata": {
        "id": "KwAVXC0V8vWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N = 2 according to the definition\n",
        "CONTEXT_SIZE = 2\n",
        "\n",
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "corpus = corpus.split()\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSvo8dncDUvv",
        "outputId": "b0d917d9-fe82-4a1d-dc5e-b6d1039bab4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an integer mapping"
      ],
      "metadata": {
        "id": "_3MOE0WmFBd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Integer word mapping\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "word_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4eHuiF8yOj",
        "outputId": "4ffbd020-0616-4a35-d4d5-a71fcafe578f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'evolve,': 0,\n",
              " 'is': 1,\n",
              " 'pattern': 2,\n",
              " 'manipulate': 3,\n",
              " 'programs': 4,\n",
              " 'are': 5,\n",
              " 'directed': 6,\n",
              " 'process': 7,\n",
              " 'People': 8,\n",
              " 'processes': 9,\n",
              " 'with': 10,\n",
              " 'other': 11,\n",
              " 'As': 12,\n",
              " 'In': 13,\n",
              " 'by': 14,\n",
              " 'processes.': 15,\n",
              " 'We': 16,\n",
              " 'process.': 17,\n",
              " 'we': 18,\n",
              " 'conjure': 19,\n",
              " 'of': 20,\n",
              " 'to': 21,\n",
              " 'Computational': 22,\n",
              " 'that': 23,\n",
              " 'direct': 24,\n",
              " 'called': 25,\n",
              " 'our': 26,\n",
              " 'create': 27,\n",
              " 'evolution': 28,\n",
              " 'spells.': 29,\n",
              " 'study': 30,\n",
              " 'inhabit': 31,\n",
              " 'about': 32,\n",
              " 'beings': 33,\n",
              " 'computational': 34,\n",
              " 'idea': 35,\n",
              " 'a': 36,\n",
              " 'things': 37,\n",
              " 'computer': 38,\n",
              " 'they': 39,\n",
              " 'computers.': 40,\n",
              " 'effect,': 41,\n",
              " 'abstract': 42,\n",
              " 'program.': 43,\n",
              " 'rules': 44,\n",
              " 'The': 45,\n",
              " 'the': 46,\n",
              " 'spirits': 47,\n",
              " 'data.': 48}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build context according to the given corpus"
      ],
      "metadata": {
        "id": "sqrPWRsTE5zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i in range(CONTEXT_SIZE, len(corpus) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [corpus[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [corpus[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = corpus[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgBAHKy5CoJ7",
        "outputId": "7f8fa471-f3e3-40cf-bf52-fa47c2607486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['are', 'We', 'to', 'study'], 'about'),\n",
              " (['about', 'are', 'study', 'the'], 'to'),\n",
              " (['to', 'about', 'the', 'idea'], 'study'),\n",
              " (['study', 'to', 'idea', 'of'], 'the'),\n",
              " (['the', 'study', 'of', 'a'], 'idea'),\n",
              " (['idea', 'the', 'a', 'computational'], 'of'),\n",
              " (['of', 'idea', 'computational', 'process.'], 'a'),\n",
              " (['a', 'of', 'process.', 'Computational'], 'computational'),\n",
              " (['computational', 'a', 'Computational', 'processes'], 'process.'),\n",
              " (['process.', 'computational', 'processes', 'are'], 'Computational'),\n",
              " (['Computational', 'process.', 'are', 'abstract'], 'processes'),\n",
              " (['processes', 'Computational', 'abstract', 'beings'], 'are'),\n",
              " (['are', 'processes', 'beings', 'that'], 'abstract'),\n",
              " (['abstract', 'are', 'that', 'inhabit'], 'beings'),\n",
              " (['beings', 'abstract', 'inhabit', 'computers.'], 'that'),\n",
              " (['that', 'beings', 'computers.', 'As'], 'inhabit'),\n",
              " (['inhabit', 'that', 'As', 'they'], 'computers.'),\n",
              " (['computers.', 'inhabit', 'they', 'evolve,'], 'As'),\n",
              " (['As', 'computers.', 'evolve,', 'processes'], 'they'),\n",
              " (['they', 'As', 'processes', 'manipulate'], 'evolve,'),\n",
              " (['evolve,', 'they', 'manipulate', 'other'], 'processes'),\n",
              " (['processes', 'evolve,', 'other', 'abstract'], 'manipulate'),\n",
              " (['manipulate', 'processes', 'abstract', 'things'], 'other'),\n",
              " (['other', 'manipulate', 'things', 'called'], 'abstract'),\n",
              " (['abstract', 'other', 'called', 'data.'], 'things'),\n",
              " (['things', 'abstract', 'data.', 'The'], 'called'),\n",
              " (['called', 'things', 'The', 'evolution'], 'data.'),\n",
              " (['data.', 'called', 'evolution', 'of'], 'The'),\n",
              " (['The', 'data.', 'of', 'a'], 'evolution'),\n",
              " (['evolution', 'The', 'a', 'process'], 'of'),\n",
              " (['of', 'evolution', 'process', 'is'], 'a'),\n",
              " (['a', 'of', 'is', 'directed'], 'process'),\n",
              " (['process', 'a', 'directed', 'by'], 'is'),\n",
              " (['is', 'process', 'by', 'a'], 'directed'),\n",
              " (['directed', 'is', 'a', 'pattern'], 'by'),\n",
              " (['by', 'directed', 'pattern', 'of'], 'a'),\n",
              " (['a', 'by', 'of', 'rules'], 'pattern'),\n",
              " (['pattern', 'a', 'rules', 'called'], 'of'),\n",
              " (['of', 'pattern', 'called', 'a'], 'rules'),\n",
              " (['rules', 'of', 'a', 'program.'], 'called'),\n",
              " (['called', 'rules', 'program.', 'People'], 'a'),\n",
              " (['a', 'called', 'People', 'create'], 'program.'),\n",
              " (['program.', 'a', 'create', 'programs'], 'People'),\n",
              " (['People', 'program.', 'programs', 'to'], 'create'),\n",
              " (['create', 'People', 'to', 'direct'], 'programs'),\n",
              " (['programs', 'create', 'direct', 'processes.'], 'to'),\n",
              " (['to', 'programs', 'processes.', 'In'], 'direct'),\n",
              " (['direct', 'to', 'In', 'effect,'], 'processes.'),\n",
              " (['processes.', 'direct', 'effect,', 'we'], 'In'),\n",
              " (['In', 'processes.', 'we', 'conjure'], 'effect,'),\n",
              " (['effect,', 'In', 'conjure', 'the'], 'we'),\n",
              " (['we', 'effect,', 'the', 'spirits'], 'conjure'),\n",
              " (['conjure', 'we', 'spirits', 'of'], 'the'),\n",
              " (['the', 'conjure', 'of', 'the'], 'spirits'),\n",
              " (['spirits', 'the', 'the', 'computer'], 'of'),\n",
              " (['of', 'spirits', 'computer', 'with'], 'the'),\n",
              " (['the', 'of', 'with', 'our'], 'computer'),\n",
              " (['computer', 'the', 'our', 'spells.'], 'with')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2\n",
        "Name at least 2 limitations at this context construction step? Explain your answers."
      ],
      "metadata": {
        "id": "EbFTMNAGWygb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize context"
      ],
      "metadata": {
        "id": "RYMlC5FSHSqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context: List[str],\n",
        "                        word_to_idx: dict) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to map a word context vector into a torch tensor\n",
        "\n",
        "    Args:\n",
        "    context (List[str]) -- A context (including individual n-grams tokens)\n",
        "    word_to_idx (dict)  -- A functionto map a word into its respective integer\n",
        "\n",
        "    Returns:\n",
        "    A pytorch tensor including a list of mapped word\n",
        "\n",
        "    Example:\n",
        "    ['are', 'We', 'to', 'study'] --> tensor([40, 22, 27, 47])\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "9aRCDPmYEnMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional test\n",
        "print(\"Example sample: \", data[0][0])\n",
        "make_context_vector(data[0][0], word_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmf-xZkEFOfZ",
        "outputId": "96784ab6-8c41-4bdf-dacb-f23245314411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example sample:  ['are', 'We', 'to', 'study']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 16, 21, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW model implementation"
      ],
      "metadata": {
        "id": "-h_4P68vHVOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Neural weight initialization\n",
        "        nn.init.xavier_normal_(self.embedding_layer.weight)\n",
        "        nn.init.xavier_normal_(self.linear_layer.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Function to conduct forward passing\n",
        "        \"\"\"\n",
        "        embedding = self.embedding_layer(inputs)\n",
        "        embedding = torch.sum(embedding, dim=1)\n",
        "        output = self.linear_layer(embedding)\n",
        "        output_softmax = F.log_softmax(output, dim=1)\n",
        "        return output_softmax"
      ],
      "metadata": {
        "id": "ViLponY1HZoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = CBOW(vocab_size=vocab_size,\n",
        "                  embed_dim=10)\n",
        "\n",
        "# Enable gradient for model training\n",
        "cbow_model.train()\n",
        "cbow_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_vTUt8aJiEn",
        "outputId": "d9600ad7-9c9e-48b1-8bb9-be49a4863c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embedding_layer): Embedding(49, 10)\n",
              "  (linear_layer): Linear(in_features=10, out_features=49, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "s9aXrzRCKQel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameters and training configuration"
      ],
      "metadata": {
        "id": "L5tYJoeKMmuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs: int = 5\n",
        "learning_rate: float = 5e-2\n",
        "optimizer: torch.optim = torch.optim.Adam(cbow_model.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "loss_function = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "UxbOAYCDLlrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "vgn6aHDsWk2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Construct input and target tensor\n",
        "    input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
        "    input_vector = input_vector.unsqueeze(0)\n",
        "    target_vector = target_vector.unsqueeze(0)\n",
        "\n",
        "    # Join whole data into 1 tensor set\n",
        "    for idx in range(1, len(data)):\n",
        "        input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n",
        "        target_tensor = torch.tensor(word_to_idx[data[idx][1]]).unsqueeze(0)\n",
        "        torch.cat((input_vector, input_tensor), 0)\n",
        "        torch.cat((target_vector, target_tensor), 0)\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "    cbow_model.zero_grad()\n",
        "\n",
        "    # Forward passing\n",
        "    log_probabilities = cbow_model(input_vector)\n",
        "\n",
        "    # Evaluate loss\n",
        "    loss = loss_function(log_probabilities, target_vector)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "    print(\"Loss:\", epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bv0CVQZKS7W",
        "outputId": "9670abaf-b513-460f-c129-0764a38009a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Epoch 1/5\n",
            "Loss: 3.928410530090332\n",
            "#Epoch 2/5\n",
            "Loss: 3.2563564777374268\n",
            "#Epoch 3/5\n",
            "Loss: 2.6207239627838135\n",
            "#Epoch 4/5\n",
            "Loss: 1.9057542085647583\n",
            "#Epoch 5/5\n",
            "Loss: 1.109306812286377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-b784ee0472f1>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
            "<ipython-input-23-b784ee0472f1>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference"
      ],
      "metadata": {
        "id": "7DrF_htiSXmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # No gradient update in inference\n",
        "    context = ['In', 'processes.', 'we', 'conjure']\n",
        "\n",
        "    # Vectorize input from text to numeric type\n",
        "    input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n",
        "\n",
        "    # Model makes prediction\n",
        "    output_tensor = cbow_model(input_tensor)\n",
        "\n",
        "    # Get the item id with the highest probability\n",
        "    prediction = torch.argmax(output_tensor).detach().tolist()\n",
        "\n",
        "    # Query the respective word from the given item id\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = key_list[prediction]\n",
        "\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3SBZo4LSDP5",
        "outputId": "de69eb87-1f11-42a8-a44f-5c2d0d3d12f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: ['In', 'processes.', 'we', 'conjure']\n",
            "Prediction: about\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-1e51cdee9400>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip-gram\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningcoban.com/tabml_book/_images/word2vec2.png\">\n",
        "</center>\n",
        "\n",
        "- Skip gram is based on the distributional hypothesis where words with similar distribution is considered to have similar meanings. Researchers of skip gram suggested a model with less parameters along with the novel methods to make optimization step more efficient.\n",
        "\n",
        "- Vanilla SkipGram model:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/a1d083c872e848787cb572a73d97f2c24947a374/5-Figure1-1.png\" scale=70%>\n",
        "</center>\n",
        "\n",
        "- Main idea is to optimize model so that if it is queried with a word, it should correctly guess all the context (context = 2 in the figure) words. That is,\n",
        "$$\n",
        "y=\\sigma(Ux)\n",
        "$$\n",
        "    - where $x$, $y$ are one-hot encoded word vector, $U$ is the embedding matrix, and $\\sigma(\\cdot)$ is the softmax function.\n",
        "\n",
        "With the same dataset, training set for skip gram can be much larger than that of NPLM since it can have $2c$ samples $\\left(w_t:w_{t-c}, ...,w_t:w_{t-1},w_t:w_{t+1},...,w_{t+c}\\right)$ while other n-gram based models have one $\\left((w_{t-c},...w_{t-1},w_{t+1},...,w_{t+c}):w_t\\right)$."
      ],
      "metadata": {
        "id": "X9FwHWmGaM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\""
      ],
      "metadata": {
        "id": "b9XRCWEvEOWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model construction\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        ### START YOUR CODE HERE ###\n",
        "        # Declare embedding function u and v\n",
        "        # with given vocab size and embed dim using nn.Embedding\n",
        "        self.v_embedding_layer = ...\n",
        "        self.u_embedding_layer = ...\n",
        "\n",
        "        # Network weight initialization with Xavier initialization\n",
        "\n",
        "\n",
        "        ### END YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, center_words, context):\n",
        "        \"\"\"\n",
        "        Function to perform forward passing\n",
        "        \"\"\"\n",
        "        v_embedding = self.v_embedding_layer(center_words)\n",
        "        u_embedding = self.u_embedding_layer(context)\n",
        "\n",
        "        score = torch.mul(v_embedding, u_embedding)\n",
        "        score = torch.sum(score, dim=1)\n",
        "        log_score = F.logsigmoid(score)\n",
        "        return log_score"
      ],
      "metadata": {
        "id": "S1dwgXNxZ_Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_model = SkipGramModel(vocab_size=vocab_size,\n",
        "                               embed_dim=128)\n",
        "\n",
        "skipgram_model.train()\n",
        "skipgram_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ls_wI-7nS1",
        "outputId": "f69f5496-261f-48c2-8ead-f4b20dca100f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SkipGramModel(\n",
              "  (v_embedding_layer): Embedding(49, 128, sparse=True)\n",
              "  (u_embedding_layer): Embedding(49, 128, sparse=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare training data to match the format of SkipGram model"
      ],
      "metadata": {
        "id": "oiBCaJJJCMr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gather_training_data(corpus,\n",
        "                         word_to_idx: dict,\n",
        "                         context_size: int):\n",
        "    \"\"\"\n",
        "    This function is to transform the given corpus\n",
        "    into the correct format for SkipGram to serve as its input\n",
        "    \"\"\"\n",
        "\n",
        "    training_data = []\n",
        "    all_vocab_indices = list(range(len(word_to_idx)))\n",
        "\n",
        "    split_text = corpus.split('\\n')\n",
        "\n",
        "    # For each sentence\n",
        "    for sentence in split_text:\n",
        "        indices = []\n",
        "        indices = [word_to_idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "        # For each word treated as center word\n",
        "        for center_word_pos in range(len(indices)):\n",
        "\n",
        "            # For each window  position\n",
        "            for w in range(-context_size, context_size+1):\n",
        "                context_word_pos = center_word_pos + w\n",
        "\n",
        "                # Make sure we dont jump out of the sentence\n",
        "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                    continue\n",
        "\n",
        "                context_word_idx = indices[context_word_pos]\n",
        "                center_word_idx  = indices[center_word_pos]\n",
        "\n",
        "                # Same words might be present in the close vicinity of each other. we want to avoid such cases\n",
        "                if center_word_idx == context_word_idx:\n",
        "                    continue\n",
        "\n",
        "                training_data.append([center_word_idx, context_word_idx])\n",
        "\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "5YlRPTuaCQBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = gather_training_data(corpus,\n",
        "                                     word_to_idx,\n",
        "                                     context_size=2)\n",
        "training_data = torch.tensor(training_data).to(dtype=torch.long)\n",
        "training_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVjk1oJkDPXq",
        "outputId": "5c6aa12d-ea15-4ced-8254-c0d2d0687714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([212, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparamters and training configuration"
      ],
      "metadata": {
        "id": "tWEIXRlg8zmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs: int = 200\n",
        "learning_rate: float = 5e-1\n",
        "optimizer: torch.optim = torch.optim.SGD(skipgram_model.parameters(),\n",
        "                                          lr=learning_rate)"
      ],
      "metadata": {
        "id": "wIgEYDKz8Sbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training phase"
      ],
      "metadata": {
        "id": "NmG0m3jK84EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs + 1):\n",
        "    \"\"\"\n",
        "    Adapt the given CBOW training code for SkipGram\n",
        "    Following by the instruction comments, or you could do it on your own ;)\n",
        "    \"\"\"\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "    # Construct input and target tensor\n",
        "\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "\n",
        "\n",
        "    # Forward passing\n",
        "    logsoftmax_prediction = skipgram_model(inputs, targets)\n",
        "\n",
        "    # Evaluate loss (Negative log likelihood)\n",
        "    loss = torch.mean(-1 * logsoftmax_prediction)\n",
        "\n",
        "    # Backpropagation\n",
        "\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = ...\n",
        "\n",
        "    # Log result\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "        print(\"Loss:\", epoch_loss)\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhGy9vBq85PX",
        "outputId": "fe2241aa-0ede-41d4-f0b0-91fcec8aa755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Epoch 0/200\n",
            "Loss: 0.6883206367492676\n",
            "#Epoch 50/200\n",
            "Loss: 0.6027075052261353\n",
            "#Epoch 100/200\n",
            "Loss: 0.5209851861000061\n",
            "#Epoch 150/200\n",
            "Loss: 0.44042086601257324\n",
            "#Epoch 200/200\n",
            "Loss: 0.364051878452301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "-X8DX0BfFDsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context = ['we']\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Based on the given inference code in the previous section, training code and the context\n",
        "    # Implement the inference flow from the given context to an output word\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE HERE ###\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ],
      "metadata": {
        "id": "U2EcHAqVFEzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "What are the differences between CBOW and Skip-gram?"
      ],
      "metadata": {
        "id": "6sg4EaBpw-De"
      }
    }
  ]
}